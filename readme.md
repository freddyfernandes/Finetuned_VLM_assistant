# InterLMM: Factory Visual Inspection Demo

![System Preview](UI_image/image.png)


This repository contains the implementation for our paper **â€œInterLMM: A Fine-tuned Large Vision-Language Model to Generate Human-Readable Explainable AI Results in the Industrial Domainâ€**, submitted to **CIRP ICME 2025**. It provides a simple Gradio notebook demo that lets non-technical users receive natural-language explanations of XAI visualizations generated by fine-tuned VLMs (Qwen2.5-7B-VL and Idefics3-8B).

---

## ğŸ“‚ Repository Structure

```
Paper_implementation/
â”œâ”€â”€ demo.ipynb                # Notebook with the Gradio interface
â”œâ”€â”€ app.py                    # Standalone Gradio demo script
â”œâ”€â”€ requirements.txt          # Python dependencies
â”œâ”€â”€ qwen/                     # Qwen2.5-7B adapter files
â”‚   â”œâ”€â”€ adapter_config.json
â”‚   â”œâ”€â”€ adapter_model.safetensors
â”‚   â”œâ”€â”€ added_tokens.json
â”‚   â”œâ”€â”€ merges.txt
â”‚   â”œâ”€â”€ special_tokens_map.json
â”‚   â”œâ”€â”€ tokenizer_config.json
â”‚   â”œâ”€â”€ tokenizer.json
â”‚   â”œâ”€â”€ training_args.bin
â”‚   â””â”€â”€ vocab.json
â””â”€â”€ idefics/                  # Idefics3-8B adapter files
    â””â”€â”€ (same structure as qwen/)
```

---

## ğŸš€ Demo Overview

The demo lets you:
1. **Select Task**  
   - Object Detection or Classification  
2. **Choose Image Source**  
   - From a predefined folder or upload via webcam  
3. **Pick or Enter a Question**  
   - Predefined prompts or custom question  
4. **Submit & Receive**  
   - A concise, 10â€“20-word explanation of the saliency map for factory parts  

All backend inference is handled by fine-tuned Qwen2.5-7B-VL and Idefics3-8B models, targeting projection modules such as (`q_proj`, `k_proj`, `v_proj`). Both adapters were trained on a custom, synthetically augmented dataset and showed improved performance on our benchmarking suite.

---

## âš™ï¸ Requirements

Create a Python â‰¥ 3.8 environment and install:

```bash
pip install -r requirements.txt
```

---

## ğŸ“¥ Installation & Setup

1. **Clone this repo**  
   ```bash
   git clone https://github.com/freddyfernandes/Finetuned_VLM_assistant.git
   cd Finetuned_VLM_assistant/Paper_implementation
   ```

2. **Install dependencies**  
   ```bash
   python -m venv venv
   source venv/bin/activate         # Linux/macOS
   # or `.env\Scriptsctivate`   # Windows
   pip install -r requirements.txt
   ```

3. **Download base models**  
   ```python
   # In Python REPL or a script:
   from transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor
   Qwen2_5_VLForConditionalGeneration.from_pretrained("Qwen/Qwen2.5-VL-7B-Instruct")
   AutoProcessor.from_pretrained("Qwen/Qwen2.5-VL-7B-Instruct")
   ```
---
   ```python
   # Similarly for Idefics3
    from transformers import AutoProcessor, AutoModelForImageTextToText
    processor = AutoProcessor.from_pretrained("HuggingFaceM4/Idefics3-8B-Llama3")
    model = AutoModelForImageTextToText.from_pretrained("HuggingFaceM4/Idefics3-8B-Llama3")
   ```
---

## ğŸ”§ Customization

- **INPUT_DIR**  
  Change the path in the notebook to point to your own saliency-map images.  
- **ADAPTER_PATH**  
  Ensure `ADAPTER_PATH` matches the `qwen/` or `idefics/` folder name.  
- **Tasks & Prompts**  
  Edit `TASKS` and `PREDEF_QS` to add or modify questions.

---

## ğŸ“ Contact

For questions, issues, or contributions, please reach out to   
Freddy Fernandes â€¢ ğŸ“§ freddyfernandesuni@gmail.com

---

**Enjoy exploring human-level XAI explanations with this simple demo!**