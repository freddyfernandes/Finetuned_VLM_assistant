{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from typing import Optional\n",
    "\n",
    "import gradio as gr\n",
    "import torch\n",
    "from transformers import AutoProcessor, Qwen2_5_VLForConditionalGeneration\n",
    "from transformers.image_utils import load_image\n",
    "from qwen_vl_utils import process_vision_info\n",
    "\n",
    "# ---------- CONFIG -----------------------------------------------------------\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "MODEL_NAME = \"Qwen/Qwen2.5-VL-7B-Instruct\" # or \"HuggingFaceM4/Idefics3-8B-Llama3\"\n",
    "INPUT_DIR = \"output_images\"\n",
    "ADAPTER_PATH: Optional[str] = 'qwen' # or idefics\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "model = None  # will be lazily initialised once\n",
    "processor = None\n",
    "\n",
    "# ---------- PREDEFINED QUESTIONS ------------------------------------------------\n",
    "TASKS = [\"Object Detection\", \"Classification\"]\n",
    "PREDEF_QS = {\n",
    "    \"Object Detection\": [\n",
    "        \"How is this part identified despite the strong metallic background?\",\n",
    "        \"Which details enable the model to identify this part?\",\n",
    "        \"Do the other metallic parts in the scene affect the prediction?\",\n",
    "        \"Which grooves, ridges, or holes does the model highlight most prominently?\",\n",
    "    ],\n",
    "    \"Classification\": [\n",
    "        \"Which texture cues guide the model's prediction?\",\n",
    "        \"What part shape influences the class label most?\",\n",
    "        \"Does background affect the prediction?\",\n",
    "        \"How robust is the classification to lighting?\",\n",
    "    ],\n",
    "}\n",
    "CUSTOM_OPT = \"Custom question...\"\n",
    "\n",
    "# ---------- MODEL LOADING HELPERS ------------------------------------------------\n",
    "def _lazy_load_model():\n",
    "    global model, processor\n",
    "    if model is None:\n",
    "        model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "            MODEL_NAME, torch_dtype=\"auto\", device_map=\"auto\"\n",
    "        )\n",
    "        if ADAPTER_PATH and ADAPTER_PATH.strip():\n",
    "            from peft import PeftModel\n",
    "            model = PeftModel.from_pretrained(model, ADAPTER_PATH)\n",
    "        processor = AutoProcessor.from_pretrained(MODEL_NAME)\n",
    "    return model, processor\n",
    "\n",
    "def _build_prompt(pil_img, img_name, class_id, question):\n",
    "    system_prompt = f\"\"\"\n",
    "    Context:\n",
    "    We are analyzing a single saliency map visualization for the image named '{img_name}', focusing on the component called '{class_id}'. \n",
    "    This image helps us understand how the model perceives and identifies the object. The visualization includes:\n",
    "    • Bounding Box: It encases the object, showing its location and providing insight into the model spatial awareness\n",
    "    • Saliency Map: It highlights the most influential pixels in the model decision-making, with brighter areas indicating higher impact on detecting '{class_id}'.\n",
    "\n",
    "    Your Task:\n",
    "    Analyze step by step how the saliency map visualization contributes to understanding the detection of '{class_id}' and provide a detailed answer to the question below:\n",
    "    Instructions for the Response:\n",
    "    Provide a clear and simple explanation suitable for non-technical workers.\n",
    "    Use straightforward language to describe how the saliency map (with its bounding box) shows the detection of '{class_id}'.\n",
    "    Limit your final response to 10 to 20 words.\n",
    "    Avoid technical jargon and complex terminology.\n",
    "    Example: “The model focuses on the straight edges, sharp corners, and the unique rectangular shape of the object.”\n",
    "    \"\"\"\n",
    "    return [\n",
    "        {\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": system_prompt.strip()}]},\n",
    "        {\"role\": \"user\", \"content\": [{\"type\": \"image\", \"image\": pil_img}, {\"type\": \"text\", \"text\": question}]},\n",
    "    ]\n",
    "\n",
    "\n",
    "@torch.inference_mode()\n",
    "def ask_model_eval(pil_img, img_name, class_id, question):\n",
    "    model, processor = _lazy_load_model()\n",
    "    msgs = _build_prompt(pil_img, img_name, class_id, question)\n",
    "    text = processor.apply_chat_template(msgs, tokenize=False, add_generation_prompt=True)\n",
    "    image_inputs, video_inputs = process_vision_info(msgs)\n",
    "    inputs = processor(\n",
    "        text=[text], images=image_inputs, videos=video_inputs,\n",
    "        padding=True, return_tensors=\"pt\"\n",
    "    ).to(model.device)\n",
    "    gen_ids = model.generate(**inputs, max_new_tokens=300)\n",
    "    trimmed = [out[len(inp):] for inp, out in zip(inputs.input_ids, gen_ids)]\n",
    "    return processor.batch_decode(trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0].strip()\n",
    "\n",
    "# ---------- UI HELPERS ------------------------------------------------------------\n",
    "def _sample_images():\n",
    "    \"\"\"\n",
    "    Return a naturally sorted list of image filenames from INPUT_DIR.\n",
    "    \"\"\"\n",
    "    files = [f for f in os.listdir(INPUT_DIR) if f.lower().endswith((\".png\", \".jpg\", \".jpeg\"))]\n",
    "    # natural sort by numeric components\n",
    "    def numeric_key(fn: str):\n",
    "        nums = re.findall(r\"(\\d+)\", fn)\n",
    "        return [int(n) for n in nums] or [fn.lower()]\n",
    "    return sorted(files, key=numeric_key)\n",
    "\n",
    "@torch.inference_mode()\n",
    "def _preview_image(filename):\n",
    "    if not filename:\n",
    "        return None\n",
    "    return load_image(os.path.join(INPUT_DIR, filename))\n",
    "\n",
    "@torch.inference_mode()\n",
    "def _infer(task, mode, filename, webcam_img, q_choice, custom_q):\n",
    "    # determine final question\n",
    "    question = custom_q.strip() if q_choice == CUSTOM_OPT else q_choice\n",
    "    # validate inputs\n",
    "    if mode == \"Select from list\" and not filename:\n",
    "        return None, \"\", \"**Please select an image.**\"\n",
    "    if mode == \"Live Inference\" and webcam_img is None:\n",
    "        return None, \"\", \"**Please capture or upload an image.**\"\n",
    "    if not question:\n",
    "        return None, \"\", \"**Please enter a question.**\"\n",
    "    # load image\n",
    "    if mode == \"Select from list\":\n",
    "        pil_img = load_image(os.path.join(INPUT_DIR, filename))\n",
    "        img_name = filename\n",
    "    else:\n",
    "        pil_img = webcam_img\n",
    "        img_name = \"live_image\"\n",
    "    # infer\n",
    "    answer = ask_model_eval(pil_img, img_name, task, question)\n",
    "    return pil_img, f\"**Answer:** {answer}\", \"\"\n",
    "\n",
    "# ---------- BUILD DEMO -----------------------------------------------------------\n",
    "with gr.Blocks(title=\"Factory Visual Inspection Demo\", theme=gr.themes.Soft()) as demo:\n",
    "    gr.Markdown(\"## Visual Inspection Assistant\")\n",
    "    gr.Markdown(\"*Choose task, image, and question before submitting.*\")\n",
    "\n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=1):\n",
    "            task_dd = gr.Dropdown(TASKS, label=\"Task\", value=TASKS[0])\n",
    "            mode_rg = gr.Radio([\"Select from list\", \"Live Inference\"], label=\"Image Source\", value=\"Select from list\")\n",
    "\n",
    "            file_dd = gr.Dropdown(_sample_images(), label=\"Sample Images\")\n",
    "            webcam_in = gr.Image(label=\"Live / Upload Image\", type=\"pil\", visible=False)\n",
    "\n",
    "            # question selector with custom option\n",
    "            q_dd = gr.Dropdown(PREDEF_QS[TASKS[0]] + [CUSTOM_OPT], label=\"Question\")\n",
    "            q_txt = gr.Textbox(label=\"Custom Question\", placeholder=\"Type your question here...\", visible=False)\n",
    "\n",
    "            run_btn = gr.Button(\"Submit\")\n",
    "            msg_out = gr.Markdown(\"\")\n",
    "\n",
    "        with gr.Column(scale=1):\n",
    "            img_out = gr.Image(label=\"Selected Image\", height=300)\n",
    "            answer_md = gr.Markdown(\"\")\n",
    "\n",
    "    # ---------- EVENTS ---------------------------------------------------------\n",
    "    task_dd.change(\n",
    "        fn=lambda t: (gr.update(choices=PREDEF_QS[t] + [CUSTOM_OPT], value=PREDEF_QS[t][0]), gr.update(visible=False, value=\"\")),\n",
    "        inputs=task_dd,\n",
    "        outputs=[q_dd, q_txt]\n",
    "    )\n",
    "    mode_rg.change(\n",
    "        fn=lambda m: (gr.update(visible=m == \"Select from list\"), gr.update(visible=m == \"Live Inference\")),\n",
    "        inputs=mode_rg,\n",
    "        outputs=[file_dd, webcam_in]\n",
    "    )\n",
    "    file_dd.change(fn=_preview_image, inputs=file_dd, outputs=img_out)\n",
    "    q_dd.change(\n",
    "        fn=lambda q: gr.update(visible=q == CUSTOM_OPT),\n",
    "        inputs=q_dd,\n",
    "        outputs=q_txt\n",
    "    )\n",
    "    \n",
    "    run_btn.click(\n",
    "        fn=_infer,\n",
    "        inputs=[task_dd, mode_rg, file_dd, webcam_in, q_dd, q_txt],\n",
    "        outputs=[img_out, answer_md, msg_out]\n",
    "    )\n",
    "##11_box, #9_rod\n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch(inline=True, share=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_qwen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
